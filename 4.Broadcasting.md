# Broadcasting自动传播

## CATALOG

1. [Broadcasting自动传播](#broadcasting自动传播)
   1. [CATALOG](#catalog)
   2. [假设情景1](#假设情景1)
   3. [假设情景2](#假设情景2)
   4. [具体用途](#具体用途)

## 假设情景1

- `[class,students,score]`的数据结构
- 对于每一个学生的成绩加5分
- [4,32,8] + [1]

生成一个10个班级，每个班32个学生，每个学生8项成绩的tensor

```Python
a = torch.randn(10,32,8)

a.shape
Out[35]: torch.Size([10, 32, 8])
```

生成需要添加的分数（针对于每一个学生）

```Python
b = torch.tensor([5])
b = b.expand(8)
b.shape
Out[39]: torch.Size([8])
```

使用`unsqueeze()`对b进行维度扩张

```Python
b = b.unsqueeze(0).unsqueeze(0)
b.shape
Out[41]: torch.Size([1, 1, 8])
```

使用`expand()`进行扩展

```Python
b = b.expand(a.shape) # 扩展为与a相同的大小tensor

b.shape
Out[45]: torch.Size([10, 32, 8])
```

数据相加

```Python
a = a + b
# 完成对每个学生的所有成绩加5
```

## 假设情景2

- `[class,students,score]`的数据结构
- 只对每个学生的英语成绩加5分
- [4,32,8] + [8]

必须指定`[8]`大小的一维tensor来确定在哪里进行相加

```Python
b = torch.tensor([0,0,0,5,0,0,0,0]) # 指定具体加在哪里

b.shape
Out[52]: torch.Size([8])

b = b.unsqueeze(0).unsqueeze(0).expand(a.shape)
# 将维度扩展和expand操作一起进行

b.shape
Out[54]: torch.Size([10, 32, 8])

a = a + b
```

## 具体用途

1. **简化代码**：Broadcasting机制允许用户直接对形状不完全相同的张量进行运算，而无需编写额外的代码来显式地调整张量的形状。这减少了代码量，提高了代码的可读性和可维护性。

2. **提高运算效率**：通过自动扩展张量的形状，Broadcasting机制能够避免不必要的数据复制和形状调整操作，从而提高了运算效率。特别是在处理大规模数据时，这种效率提升尤为明显。

3. **支持灵活的操作**：Broadcasting机制支持多种元素级运算，如加法、减法、乘法、除法等，使得用户可以在不同形状的张量之间灵活地进行这些操作。
